{
  "document_id": "Hamza_CV_Updated",
  "timestamp": "2025-09-04T20:08:06.858343",
  "chunk_count": 11,
  "chunks": [
    {
      "chunk_id": "f924fa38dd8aa01e",
      "document_id": "Hamza_CV_Updated",
      "content": "# Processed Document: Hamza_CV_Updated.pdf\n\n**Document ID**: Hamza_CV_Updated\n**Pages**: 3\n**Processing Date**: 2025-09-04 20:07:46\n**Content Length**: 10,253 characters\n\n---\n\n## Document Content with AI Vision Enhancement\n\n## Hamza Khaled Mahmoud Ahmed\n\nAI Engineer - Machine Learning Engineer - Data Scientist\n\nhamzakhaledlklk@gmail.com\n\ngithub.com/h19overflow - Hamza Khaled Linkedin\n\nCyberjaya, Malaysia - +60 122938594\n\n## Summary\n\nHighly motivated Computer Science student (expected graduation May 2026 ) specializing in Data Science , Machine Learning , and Artificial Intelligence , with 1.5+ years of hands-on project experience in predictive analytics , deep learning , computer vision , natural language processing , and AI automation systems . Possesses a robust foundation in statistics , advanced mathematics , data analysis , algorithm development , and predictive modeling .",
      "page_number": 1,
      "chunk_index": 0,
      "metadata": {
        "chunk_length": 890,
        "word_count": 120,
        "created_at": "2025-09-04T20:08:06.857338",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\Hamza_CV_Updated\\Hamza_CV_Updated_processed.md",
        "original_filename": "Hamza_CV_Updated_processed",
        "document_type": "other",
        "chunk_position": "start"
      }
    },
    {
      "chunk_id": "e0b7c65b3ebb4c5b",
      "document_id": "Hamza_CV_Updated",
      "content": "Proven ability to manage the end-to-end machine learning lifecycle :\n\n- Data engineering , preprocessing, feature engineering , and data pipeline development\n- Exploratory data analysis (EDA), statistical inference , and hypothesis testing\n- Model development (classical ML, deep learning, neural networks, LLMs, RAG, AI agents), training , and hyperparameter optimization\n- Model evaluation , performance tuning , cross-validation , and deployment strategies\n\nPassionate about applying AI/ML to build intelligent, high-impact solutions.",
      "page_number": 1,
      "chunk_index": 1,
      "metadata": {
        "chunk_length": 537,
        "word_count": 72,
        "created_at": "2025-09-04T20:08:06.857338",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\Hamza_CV_Updated\\Hamza_CV_Updated_processed.md",
        "original_filename": "Hamza_CV_Updated_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "d2ba64c1a820eb0d",
      "document_id": "Hamza_CV_Updated",
      "content": "Demonstrated project success in Federated Learning , Advanced RAG Systems , AI Inference Optimization , Full-Stack AI Development , and Computer Vision . Seeking an entry-level Machine Learning Engineer , AI Engineer , or Data Scientist position to contribute to data-driven projects in FinTech , Healthcare , or AI/Tech . ## Education\n\nBSc in Computer Science (Specialization in Data Science)\n\nMarch 2023 - Present\n\nMultimedia University, Malaysia\n\nExpected Graduation:\n\nMay 2026\n\nCGPA:\n\n3.63 / 4.0\n\nAchievements:\n\n4-time Dean's List Award Winner\n\nRelevant Coursework: Statistics, Calculus, Discrete Mathematics, Machine Learning Algorithms , Data Analysis, Deep Learning , Object-Oriented Pro- gramming, Object-Oriented Analysis &amp; Design, Database Management, Artificial Intelligence .",
      "page_number": 2,
      "chunk_index": 2,
      "metadata": {
        "chunk_length": 791,
        "word_count": 108,
        "created_at": "2025-09-04T20:08:06.857338",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\Hamza_CV_Updated\\Hamza_CV_Updated_processed.md",
        "original_filename": "Hamza_CV_Updated_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "053d0d405b729719",
      "document_id": "Hamza_CV_Updated",
      "content": "## Selected Projects\n\nFederated Clinical Trial Matching Platform - Privacy-Preserving AI in Healthcare Technologies: Python, NVIDIA FLARE, MedGemma, vLLM, MongoDB Atlas, RAG, PyTorch\n\n- Architecting a novel, privacy-preserving clinical trial matching platform using Federated Learning (NVIDIA FLARE) to identify eligible patients across multiple hospitals without centralizing sensitive data. - Designed an efficient two-stage matching pipeline : a RAG-based candidate retrieval system using hybrid vector/SQL search, followed by a high-accuracy validation stage with a specialized medical LLM ( MedGemma ).\n\n- Currently implementing advanced inference optimizations with vLLM , leveraging Paged Attention and dynamic batching to dramatically increase throughput and reduce the latency of the MedGemma validation service. - Engineered a robust, offline ETL pipeline to periodically process and embed new clinical data, ensuring the search index remains consistently up-to-date with the latest patient information. ## Graph-Powered Agentic RAG System - Advanced AI Research &amp; Development\n\nTechnologies: Python, LightRAG, LangGraph, Google Gemini API (Vision Pro &amp; Flash), PostgreSQL, Pydantic, MinerU\n\n- Architected and built an end-to-end Retrieval-Augmented Generation (LightRAG) system in a 3-day sprint to solve the 'fragmented context' problem inherent in traditional vector search-based RAG.",
      "page_number": 2,
      "chunk_index": 3,
      "metadata": {
        "chunk_length": 1404,
        "word_count": 185,
        "created_at": "2025-09-04T20:08:06.858343",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\Hamza_CV_Updated\\Hamza_CV_Updated_processed.md",
        "original_filename": "Hamza_CV_Updated_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "ea632506c2bba711",
      "document_id": "Hamza_CV_Updated",
      "content": "- Engineered a multi-stage, multi-modal ETL pipeline that intelligently processes complex PDFs, using a Vision Language Model (VLM) to analyze images and diagrams, and prepares the data for ingestion into a knowledge graph. - Implemented a sophisticated multi-agent system using LangGraph , where specialized AI agents collaborate to analyze user queries, determine optimal hybrid retrieval strategies (vector + graph), and synthesize fragmented answers into coherent, actionable narratives. - Demonstrated a significant leap in AI reasoning capabilities by enabling the system to understand and traverse the relationships between concepts, moving beyond simple keyword matching to structured, human-like understanding.",
      "page_number": 3,
      "chunk_index": 4,
      "metadata": {
        "chunk_length": 719,
        "word_count": 97,
        "created_at": "2025-09-04T20:08:06.858343",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\Hamza_CV_Updated\\Hamza_CV_Updated_processed.md",
        "original_filename": "Hamza_CV_Updated_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "e098f0704cf43ffc",
      "document_id": "Hamza_CV_Updated",
      "content": "## LangGraph Agentic Auditing System - AI-Powered Financial Automation\n\n## Technologies: LangGraph, Python, LLMs, Pandas, Multi-Agent Systems\n\n- Designed a multi-agent auditing system using LangGraph to automate complex financial workflows, including End-of-Service (EOS) calculations and interactive payroll analysis. - Architected stateful, graph-based workflows to manage data flow and control, enabling both fully automated processing and complex human-in-the-loop (HITL) interactions. - Deployed specialized AI agents for tasks such as data classification and dynamic transformation based on natural language instructions , functioning as an auditor's 'co-pilot'.",
      "page_number": 3,
      "chunk_index": 5,
      "metadata": {
        "chunk_length": 668,
        "word_count": 84,
        "created_at": "2025-09-04T20:08:06.858343",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\Hamza_CV_Updated\\Hamza_CV_Updated_processed.md",
        "original_filename": "Hamza_CV_Updated_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "95b14f1bc1b9ed90",
      "document_id": "Hamza_CV_Updated",
      "content": "## Agentic Workbench - Full-Stack AI Document Processing &amp; Analytics Platform\n\nTechnologies: LangGraph, LangChain, FastAPI, Google Gemini (LLM), Google Vision (OCR), SQLite, React, TypeScript, Docker, Python\n\n- Engineered a full-stack, AI-powered workbench to automate structured data extraction from documents and enable natural language-based analytics and visualizations. - Orchestrated complex, multi-agent workflows using LangGraph for an end-to-end document processing pipeline, incorporating OCR, LLM-based extraction, and a human-in-the-loop (HITL) review stage.",
      "page_number": 4,
      "chunk_index": 6,
      "metadata": {
        "chunk_length": 574,
        "word_count": 69,
        "created_at": "2025-09-04T20:08:06.858343",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\Hamza_CV_Updated\\Hamza_CV_Updated_processed.md",
        "original_filename": "Hamza_CV_Updated_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "df5cedf7fb02d8a0",
      "document_id": "Hamza_CV_Updated",
      "content": "- Developed a robust backend using FastAPI to serve RESTful APIs and WebSockets for real-time status updates, interfacing with Google Gemini for data structuring and Google Vision for OCR. ## Intelligent Customer Service Assistant with Hybrid ML/LLM Architecture\n\nTechnologies: Python, LangChain (LangGraph), Scikit-learn (Random Forest), ONNX, MongoDB Atlas, Redis\n\n- Engineered a cost-effective hybrid architecture by developing a custom Random Forest intent classifier ( 99% accuracy ), optimized with ONNX Runtime for sub-millisecond inference, reducing reliance on expensive LLM calls.",
      "page_number": 4,
      "chunk_index": 7,
      "metadata": {
        "chunk_length": 590,
        "word_count": 80,
        "created_at": "2025-09-04T20:08:06.858343",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\Hamza_CV_Updated\\Hamza_CV_Updated_processed.md",
        "original_filename": "Hamza_CV_Updated_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "82c94c2083b3474a",
      "document_id": "Hamza_CV_Updated",
      "content": "- Architected an advanced agentic system using LangGraph , featuring custom subgraphs for reliable structured output and a novel 'pre-hook context fetching' mechanism to minimize token consumption. - Implemented an autonomous memory management system using MongoDB Atlas to store and retrieve user history, enabling the agent to personalize conversations and adapt its communication style. ## Pneumonia Detection using Hypertuned ResNet50V2 and Simulated Federated Learning\n\n## Technologies: TensorFlow, Keras (ResNet50V2), Flower (flwr), Deep Learning\n\n- Engineered a deep learning model for pneumonia detection from X-ray images, leveraging a fine-tuned ResNet50V2 network to achieve 95% accuracy and 0.90 F1-score .\n\n- Optimized model performance through comprehensive hyperparameter tuning and robust data augmentation strategies for improved generalization . - Designed a simulated Federated Learning environment using the Flower framework, demonstrating privacypreserving model training principles. ## AI-Powered Flashcard Generator with RAG and Web Interface\n\nTechnologies: Google Gemini API, RAG, Flask, React, FAISS, LangChain, Python, Pandas, NumPy, Vector Databases\n\n- Built an automated content generation system to create study materials from PDF documents, enhancing learning efficiency through automated document understanding .",
      "page_number": 5,
      "chunk_index": 8,
      "metadata": {
        "chunk_length": 1343,
        "word_count": 176,
        "created_at": "2025-09-04T20:08:06.858343",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\Hamza_CV_Updated\\Hamza_CV_Updated_processed.md",
        "original_filename": "Hamza_CV_Updated_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "0b7c9b613f438bd5",
      "document_id": "Hamza_CV_Updated",
      "content": "- Implemented a robust Retrieval-Augmented Generation (RAG) architecture leveraging Google's Gemini LLM and a FAISS vector database for intelligent semantic search and accurate content generation. ## Fraud Detection Model (Blockchain Transactions) - Machine Learning Classification\n\n## Technologies: XGBoost, Random Forest, Scikit-learn, Pandas, SMOTE, Ensemble Methods\n\n- Developed and evaluated multiple models for detecting fraudulent blockchain transactions, achieving 95.67% test accuracy with an optimized Random Forest model.",
      "page_number": 5,
      "chunk_index": 9,
      "metadata": {
        "chunk_length": 532,
        "word_count": 66,
        "created_at": "2025-09-04T20:08:06.858343",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\Hamza_CV_Updated\\Hamza_CV_Updated_processed.md",
        "original_filename": "Hamza_CV_Updated_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "63db46f6e71012c9",
      "document_id": "Hamza_CV_Updated",
      "content": "- Implemented SMOTE to effectively address significant class imbalance , enhancing model robustness for anomaly detection . ## Technical Skills\n\n- Programming &amp; Data Manipulation:\n- Languages: Python (intermediate), SQL (intermediate: PostgreSQL, MySQL, SQLite), Java (Intermediate), Kotlin (Intermediate)\n- Data Analysis &amp; Visualization: Pandas , NumPy , SciPy , Matplotlib , Seaborn , Tableau, Exploratory Data Analysis (EDA)\n- Machine Learning &amp; AI:\n- Core Concepts: Regression, Classification , Clustering, Ensemble Methods , Feature Engineering , Hyperparameter Optimization , Model Evaluation , Anomaly Detection\n- Deep Learning Frameworks: TensorFlow , Keras , PyTorch , Scikit-learn , XGBoost , Hugging Face Transformers , ONNX\n- Natural Language Processing (NLP): LLMs (Gemini, MedGemma), Advanced RAG , Knowledge Graphs , Structured Data Extraction , Text Embeddings , Prompt Engineering , AI Agent Systems ( LangChain , LangGraph , LightRAG , CrewAI)\n- Computer Vision: OpenCV, YOLO , MediaPipe, Object Detection, Image Classification, Optical Character Recognition (OCR)\n- Federated Learning: Flower, NVIDIA FLARE , Privacy-Preserving ML, Distributed Training\n- Inference Optimization: vLLM , Paged Attention, Model Quantization (Conceptual), Latency/Throughput Tuning\n- MLOps &amp; Cloud Platforms:\n- Cloud Platforms: Google Cloud Platform (GCP) , Vertex AI (Gemini Models)\n- Orchestration &amp; Monitoring: Prefect (Intermediate), Weights and Biases (Intermediate)\n- DevOps &amp; Tools: Docker , Git , GitHub , Jupyter Notebooks, CI/CD (Conceptual)\n- Databases &amp; Data Management:\n- Databases: SQL (PostgreSQL, MySQL), NoSQL ( MongoDB Atlas ), Vector Databases (FAISS), Redis\n- Data Concepts: Knowledge Graphs , Data Modeling, Data Warehousing (Conceptual)\n- Web Development &amp; API Integration:\n- Backend: Flask , FastAPI , RESTful APIs\n- Frontend: React , TypeScript\n- Foundations:\n- Statistical &amp; Mathematical: Statistical Modeling , Hypothesis Testing , Linear Algebra, Calculus, Probability &amp; Statistics, Optimization\n\n## Professional Competencies\n\n- Core Strengths: Fast Learning (Expert), AI Utilization (Expert), Analytical Problem-Solving , Critical Thinking\n- Collaboration: Technical Communication, Team Collaboration, Project Leadership\n- Mindset: Data-driven Decision Making , Adaptability, Continuous Learner\n\n## Languages\n\nEnglish:\n\nFluent\n\nArabic:\n\nNative",
      "page_number": 6,
      "chunk_index": 10,
      "metadata": {
        "chunk_length": 2410,
        "word_count": 325,
        "created_at": "2025-09-04T20:08:06.858343",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\Hamza_CV_Updated\\Hamza_CV_Updated_processed.md",
        "original_filename": "Hamza_CV_Updated_processed",
        "document_type": "other",
        "chunk_position": "end"
      }
    }
  ],
  "statistics": {
    "total_characters": 10458,
    "total_words": 1382,
    "avg_chunk_length": 950.7,
    "min_chunk_length": 532,
    "max_chunk_length": 2410
  }
}