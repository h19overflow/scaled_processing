{
  "document_id": "doc_20250901_185340_9b8f4443",
  "timestamp": "2025-09-01T18:54:46.554347",
  "chunk_count": 37,
  "chunks": [
    {
      "chunk_id": "ae1103e33a8346c3",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "# Processed Document: A-Comparative-Analysis-of-Leading-Language-Models-for-System-Integration.pdf\n\n**Document ID**: doc_20250901_185340_9b8f4443\n**Pages**: 15\n**Processing Date**: 2025-09-01 18:54:26\n**Content Length**: 26,472 characters\n\n---\n\n## Document Content with AI Vision Enhancement\n\nA Comparative Analysis of Leading Language Models for System Integration\n\n![Image 5](C:\\Users\\User\\Projects\\scaled_processing\\data\\documents\\raw\\extracted_images\\A-Comparative-Analysis-of-Leading-Language-Models-for-System-Integration\\picture-5.png)\n\n**AI Analysis** (confidence: 0.9): Here's a breakdown of the document image you sent:\n\n**Key Information, Data, and Insights**\n\n*   The document is a comparative analysis of leading Language Models (LLMs) for system integration. *   It identifies three LLM providers: OpenAI, Google, and Meta (hosted open-source model).",
      "page_number": 1,
      "chunk_index": 0,
      "metadata": {
        "chunk_length": 864,
        "word_count": 83,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "start"
      }
    },
    {
      "chunk_id": "09377b5e197eb4fb",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "*   It specifically names models from OpenAI (GPT-4o-Mini) and Google (Gemini 2.5 Flash). *   It notes that GPT-4o-Mini is cost-effective, swift, and the successor to GPT-3.5 Turbo. It also mentions support for text and vision inputs. *   The document suggests that there are tables, charts, figures, and diagrams within it. **Readable Text Content**\n\nThe main text content includes:\n\n*   \"A Comparative Analysis of Leading Language Models for System Integration\"\n*   \"Chapter 1: Understanding the Landscape\"\n*   \"Overview of current LLM offerings\"\n*   \"Model Descriptions\"\n*   \"Providers tested:\"\n*   \"1 - OpenAI: Gpt\"\n*   \"2 - Google: Gemini\"\n*   \"3 - Hosted Open Source Model : meta-llama\"\n*   \"GPT-4o-Mini\"\n*   \"Gemini 2.5 Flash\"\n*   \"Cost-effective, swift OpenAI model\"\n*   \"Successor to GPT-3.5 Turbo\"\n*   \"Supports text and vision inputs\"\n*   \"Open Source Model:\"\n\n**Relationship to the Document**\n\nThe text appears to be the beginning of a more comprehensive report comparing different LLMs. It outlines the scope of the analysis, the models being considered, and some key features of specific models.",
      "page_number": 1,
      "chunk_index": 1,
      "metadata": {
        "chunk_length": 1109,
        "word_count": 169,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "f4dc7969987a4483",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "The presence of indicators for tables, charts, and diagrams suggests that the full document contains more detailed comparative information. **Technical Details**\n\n*   The document focuses on LLMs, which are a type of AI model designed for natural language processing tasks. *   The models mentioned (GPT-4o-Mini, Gemini 2.5 Flash) are specific implementations of LLMs by major AI providers. *   The analysis considers cost-effectiveness, speed, and input modalities (text, vision) as relevant comparison parameters.",
      "page_number": 2,
      "chunk_index": 2,
      "metadata": {
        "chunk_length": 515,
        "word_count": 72,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "a9d24e7af1500dc0",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "*   The inclusion of an open-source model indicates an interest in considering models that can be hosted and customized independently. ![Image 7](C:\\Users\\User\\Projects\\scaled_processing\\data\\documents\\raw\\extracted_images\\A-Comparative-Analysis-of-Leading-Language-Models-for-System-Integration\\picture-7.png)\n\n**AI Analysis** (confidence: 0.9): Here's a breakdown of the information presented in the image and its likely context within the larger document:\n\n**Key Information, Data, and Insights:**\n\n*   **High-Level AI Model Evaluation Architecture:** The image outlines a system designed to objectively evaluate multiple AI models.",
      "page_number": 2,
      "chunk_index": 3,
      "metadata": {
        "chunk_length": 635,
        "word_count": 67,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "0c3a3e8fc057f07d",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "*   **Inputs:** A \"Dataset\" containing reading comprehension questions with multiple-choice answers serves as the input to the AI models. The questions and context are extracted from the dataset. *   **AI Models:**  Multiple AI providers feed into the system. The image specifically mentions:\n    *   OpenAI 4o-mini\n    *   Gemini 2.5 Flash\n    *   Gemini 2.5 Flash Lite\n    Each of these models generates an \"Answer with reasoning\" and a \"Teaching explanation\" in response to the input. *   **Evaluation System:** The core of the evaluation process is the \"Gemini 2.5 Pro Impartial Judge.\" This component evaluates the \"Generated Responses\" from the various AI models.",
      "page_number": 3,
      "chunk_index": 4,
      "metadata": {
        "chunk_length": 669,
        "word_count": 99,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "36a8b06aaf75e44a",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "*   **Evaluation Principle:** The guiding principle is \"One Judge → Multiple Models → Fair Comparison.\" This emphasizes the importance of a single, consistent evaluator for all models. The evaluation principle relies on using Gemini 2.5 Pro to ensure an objective assessment. *   **Key Design Decision:** The decision to use a \"Single impartial judge\" is highlighted as crucial. This promotes consistency in evaluation criteria across all models. *   **Bias Prevention:** The design incorporates several features to prevent bias:\n    *   No self-evaluation (the AI models are not evaluating their own responses).",
      "page_number": 3,
      "chunk_index": 5,
      "metadata": {
        "chunk_length": 612,
        "word_count": 89,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "d116f4660205c473",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "*   Standardized scoring (0.0-1.0). *   Fair comparison across providers. **Readable Text Content:**\n\nThe text content is mostly within the diagram, and it's a mix of labels, descriptions, and principles. Here's a summary:\n\n*   Title: High-Level AI Model Evaluation Architecture\n*   Inputs: Dataset, Reading comprehension questions with multiple choice answers\n*   AI Models: Multiple AI Providers, OpenAI 4o-mini, Gemini 2.5 Flash, Gemini 2.5 Flash Lite, Answer with reasoning, Teaching explanation\n*   Evaluation System: Gemini 2.5 Pro Impartial Judge\n*   Evaluation Principle: One Judge → Multiple Models → Fair Comparison, Ensures objective assessment by having Gemini 2.5 Pro evaluate all responses including other Gemini variants\n*   Key Design Decision: Single impartial judge evaluates ALL models using consistent criteria\n*   Prevents bias: No self-evaluation, Standardized scoring (0.0-1.0), Fair comparison across providers\n\n**Relationship to Document:**\n\nThe document seems to be a comparative analysis of leading language models for system integration.",
      "page_number": 4,
      "chunk_index": 6,
      "metadata": {
        "chunk_length": 1065,
        "word_count": 146,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "72f2648ae865a8ff",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "The image likely serves as a visual aid for understanding how these models are evaluated in a consistent and unbiased manner. The preceding parts of the document indicate:\n\n*   The analysis is structured as a comparative study of LLMs. *   The document includes an overview of current LLM offerings. *   The providers and specific models under consideration are OpenAI, Google, and hosted open-source models (meta-llama). *   There are separate sections describing the models, including GPT-4o-Mini, Gemini 2.5 Flash, and an open-source model.",
      "page_number": 4,
      "chunk_index": 7,
      "metadata": {
        "chunk_length": 543,
        "word_count": 82,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "e84d915bd261622e",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "**Technical Details:**\n\n*   The evaluation system involves using a specific LLM, Gemini 2.5 Pro, as the impartial judge. *   Models are evaluated based on their ability to provide answers with reasoning and teaching explanations. *   The scoring system is standardized (0.0-1.0), suggesting a numerical scoring mechanism. *   The design aims to eliminate self-evaluation, meaning the AI models themselves do not assess their own performance. **Overall:**\n\nThe image and associated text suggest a rigorous approach to comparing language models, using a consistent evaluation framework and addressing potential biases.",
      "page_number": 5,
      "chunk_index": 8,
      "metadata": {
        "chunk_length": 616,
        "word_count": 87,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "529893d178aaf772",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "It's part of a larger document detailing a comparative analysis of leading LLMs in the context of system integration. ## Chapter 1\n\n## Understanding the Landscape\n\nOverview of current LLM offerings\n\n![Image 13](C:\\Users\\User\\Projects\\scaled_processing\\data\\documents\\raw\\extracted_images\\A-Comparative-Analysis-of-Leading-Language-Models-for-System-Integration\\picture-13.png)\n\n**AI Analysis** (confidence: 0.9): Here's a breakdown of the document content:\n\n**Key Information, Data, Insights:**\n\n*   The document is a comparative analysis of leading language models (LLMs) for system integration.",
      "page_number": 5,
      "chunk_index": 9,
      "metadata": {
        "chunk_length": 596,
        "word_count": 63,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "35fa0b41e4fe43a9",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "*   It specifically focuses on the current LLM landscape and model descriptions. *   The study includes testing providers like OpenAI (GPT), Google (Gemini), and Meta (Llama - hosted open-source model). *   Specific models mentioned are GPT-4o-Mini and Gemini 2.5 Flash. *   The GPT-4o-Mini is described as cost-effective, fast, and a successor to GPT-3.5 Turbo, supporting both text and vision inputs. **Readable Text Content:**\n\n*   \"A Comparative Analysis of Leading Language Models for System Integration\" (Title)\n*   \"Chapter 1\"\n*   \"Understanding the Landscape\"\n*   \"Overview of current LLM offerings\"\n*   \"Model Descriptions\"\n*   \"Providers tested:\"\n*   \"1 - OpenAI: Gpt\"\n*   \"2 - Google: Gemini\"\n*   \"3 - Hosted Open Source Model : meta-llama\"\n*   \"GPT-4o-Mini\"\n*   \"Gemini 2.5 Flash\"\n*   \"- Cost-effective, swift OpenAI model\"\n*   \"- Successor to GPT-3.5 Turbo\"\n*   \"- Supports text and vision inputs\"\n*   \"Open Source Model:\"\n\n**Relationship to Document:**\n\n*   The text content forms the core of the document, introducing the purpose, scope, and key players/models involved in the LLM comparison.",
      "page_number": 6,
      "chunk_index": 10,
      "metadata": {
        "chunk_length": 1107,
        "word_count": 167,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "3e9b7342cd8937d5",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "**Technical Details:**\n\n*   The document deals with language models, a branch of Artificial Intelligence and Natural Language Processing. *   It mentions GPT-4o-Mini and Gemini 2.5 Flash, indicating a focus on relatively recent and advanced models. *   The reference to \"vision inputs\" suggests the GPT-4o-Mini is a multimodal model. *   The document references open source models and providers like OpenAI, Google, and Meta. ## Model Descriptions\n\n## Providers tested:\n\n## 1 - OpenAI: Gpt\n\n## 2 - Google: Gemini\n\n## 3 - Hosted Open Source Model : meta-llama\n\n## GPT-4o-Mini\n\n## Gemini 2.5 Flash\n\n- Cost-effective, swift OpenAI model\n- Successor to GPT-3.5 Turbo\n- Supports text and vision inputs\n\n## Open Source Model:\n\n- Google's balanced model: speed, quality, cost\n- Ideal for user-facing applications\n- \"Thinking\" capabilities for reasoning\n\n1\n\n## meta-llama/Llama-3.1-8B-Instruct\n\n- Open-source instruction-tuned model from Meta (8B parameters).",
      "page_number": 6,
      "chunk_index": 11,
      "metadata": {
        "chunk_length": 951,
        "word_count": 141,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "eff16887bce788c8",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "- Strong reasoning and language quality for its size. - Pros: Free to use, customizable, can fine-tune for domain-specific tasks. - Cons: Requires significant compute to host (GPU memory ~16324GB), infra scaling is non-trivial, monitoring/updates are on you. - Best suited for teams that want full control + on-prem/cloud deployment , but comes with higher ops overhead. ![Image 15](C:\\Users\\User\\Projects\\scaled_processing\\data\\documents\\raw\\extracted_images\\A-Comparative-Analysis-of-Leading-Language-Models-for-System-Integration\\picture-15.png)\n\n**AI Analysis** (confidence: 0.9): Here's an analysis of the document image you provided:\n\n**Key Information, Data, and Insights:**\n\n*   **Focus:** The document is a comparative analysis of leading Large Language Models (LLMs) for system integration.",
      "page_number": 7,
      "chunk_index": 12,
      "metadata": {
        "chunk_length": 800,
        "word_count": 94,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "96c3ee6671c61950",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "*   **Models Compared:** The analysis focuses on models from three major providers: OpenAI, Google, and Meta (hosted open-source). *   **Specific Models Highlighted:**\n    *   OpenAI: GPT-4o-Mini (positioned as cost-effective, swift, and a successor to GPT-3.5 Turbo)\n    *   Google: Gemini 2.5 Flash\n    *   Open Source Model: meta-llama\n*   **Features noted:**\n    *   GPT-4o-Mini supports text and vision inputs\n\n**Readable Text Content:**\n\n*   \"A Comparative Analysis of Leading Language Models for System Integration\" (Title)\n*   \"Chapter 1\"\n*   \"Understanding the Landscape\"\n*   \"Overview of current LLM offerings\"\n*   \"Model Descriptions\"\n*   \"Providers tested:\"\n*   \"1 - OpenAI: Gpt\"\n*   \"2 - Google: Gemini\"\n*   \"3 - Hosted Open Source Model : meta-llama\"\n*   \"GPT-4o-Mini\"\n*   \"Gemini 2.5 Flash\"\n*   \"- Cost-effective, swift OpenAI model\"\n*   \"- Successor to GPT-3.5 Turbo\"\n*   \"- Supports text and vision inputs\"\n*   \"Open Source Model:\"\n*   \"[Document contains tables/charts] [Document contains figures/diagrams]\"\n\n**Relationship to Document:**\n\n*   The image shows an overview of a document section that introduces and compares the LLMs from different providers.\n\n*   The document appears to be well-structured, with chapters and clearly defined sections. *   The inclusion of tables, charts, figures, and diagrams suggests a thorough and data-driven approach to the analysis. **Technical Details:**\n\n*   **LLMs:** The document revolves around Large Language Models (LLMs), which are deep learning models trained on massive amounts of text data to understand and generate human-like language. *   **Providers:** OpenAI, Google, and Meta are leading providers of LLMs.",
      "page_number": 7,
      "chunk_index": 13,
      "metadata": {
        "chunk_length": 1680,
        "word_count": 238,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "de01f6b65e53c7e4",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "*   **Model Versions:** The document specifically mentions GPT-4o-Mini and Gemini 2.5 Flash, suggesting that the analysis is focused on newer or specific versions of the models. *   **Multimodal Capabilities:** The mention of \"supports text and vision inputs\" indicates that some models (e.g., GPT-4o-Mini) can process both text and image data, enabling more complex applications. ## Gemini 2.0 Flash\n\n- Combining stability and accuracy , and the most cost effective and used model in production. - High volume , low latency , and moderate accuracy for tasks that require reasoning , excels in text based tasks.",
      "page_number": 8,
      "chunk_index": 14,
      "metadata": {
        "chunk_length": 611,
        "word_count": 95,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "eeb311bbedb5e0e7",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "## Context Window Capabilities\n\n## What is Context Window? The context window defines the amount of text a model can process, vital for long documents or conversations. ![Image 16](C:\\Users\\User\\Projects\\scaled_processing\\data\\documents\\raw\\extracted_images\\A-Comparative-Analysis-of-Leading-Language-Models-for-System-Integration\\picture-16.png)\n\n**AI Analysis** (confidence: 0.9): Here's a breakdown of the document image and its context:\n\n**Key Information and Insights:**\n\n*   **Purpose:** The document seems to be a comparative analysis of different Large Language Models (LLMs) specifically for system integration.",
      "page_number": 8,
      "chunk_index": 15,
      "metadata": {
        "chunk_length": 620,
        "word_count": 67,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "b5446f4b8ff2c64e",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "*   **Models Compared:** The document explicitly mentions OpenAI's GPT (likely various versions), Google's Gemini, and the Meta-Llama open-source model. Specific versions mentioned are GPT-4o-Mini and Gemini 2.5 Flash. *   **Evaluation Criteria:** The document highlights cost-effectiveness and speed as evaluation points. Vision input support is also mentioned for GPT-4o-Mini. *   **Structure:** The document is organized into chapters, with Chapter 1 providing an overview. There's a section specifically dedicated to model descriptions.",
      "page_number": 9,
      "chunk_index": 16,
      "metadata": {
        "chunk_length": 540,
        "word_count": 70,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "1cf86c8d2a3c23f5",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "**Readable Text Content:**\n\n*   \"A Comparative Analysis of Leading Language Models for System Integration\"\n*   \"Chapter 1: Understanding the Landscape\"\n*   \"Overview of current LLM offerings\"\n*   \"Model Descriptions\"\n*   \"Providers tested: 1 - OpenAI: Gpt, 2 - Google: Gemini, 3 - Hosted Open Source Model : meta-llama\"\n*   \"GPT-4o-Mini\"\n*   \"Gemini 2.5 Flash\"\n*   \"Cost-effective, swift OpenAI model\"\n*   \"Successor to GPT-3.5 Turbo\"\n*   \"Supports text and vision inputs\"\n*   \"Open Source Model:\"\n\n**Relationship to Document:**\n\nThe images are likely included to improve the report, as the text mentioned there are tables/charts and figures/diagrams included.",
      "page_number": 9,
      "chunk_index": 17,
      "metadata": {
        "chunk_length": 660,
        "word_count": 96,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "9a0396c05dddb77b",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "**Technical Details:**\n\n*   **LLMs:** The core subject is Large Language Models, which are AI models trained on massive datasets to understand and generate human-like text. *   **System Integration:** The analysis focuses on how well these models can be integrated into existing systems. *   **Open Source:** The inclusion of Meta-Llama suggests an evaluation of both proprietary (OpenAI, Google) and open-source LLMs. *   **GPT-4o-Mini and Gemini 2.5 Flash:** These are specific model versions, implying a comparison of state-of-the-art offerings.",
      "page_number": 10,
      "chunk_index": 18,
      "metadata": {
        "chunk_length": 548,
        "word_count": 77,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "f0472c08573aa2df",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "In essence, the document aims to provide a structured comparison of several prominent LLMs to guide decisions about system integration. Second interaction maxes out the available context length and 'pushes' the first words out of the LLMs memory\n\n## When using an AI system, how much it &lt;remembers= depends on the situation:\n\n- Chatting with the AI : The AI needs to remember past messages so it can respond in a natural way and keep track of the conversation. - Creating things like quizzes, flashcards, or questions : The AI doesn9t need to keep the history.",
      "page_number": 10,
      "chunk_index": 19,
      "metadata": {
        "chunk_length": 563,
        "word_count": 96,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "1ddb34c40ea12fa6",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "Once it generates the content, we9re done and can move on without it remembering what came before. - Adjusting the AI9s personality or style : This is somewhere in between. For example, if we want the AI to act like a tutor, we set its &lt;persona= to teacher mode and it stays that way for the whole session. If later we want it to act in a different style, like being more playful or curious, we just update the instructions and it switches modes. This matters because how the AI remembers (or forgets) shapes how useful it is4whether it9s for having a conversation, generating content, or adapting to different situations.",
      "page_number": 11,
      "chunk_index": 20,
      "metadata": {
        "chunk_length": 625,
        "word_count": 110,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "927e90a5c55d1d83",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "| Model                            | ContextWindowSize   |\n|----------------------------------|---------------------|\n| gpt-4o-mini                      | 128,000 tokens      |\n| gemini 2.5 flash                 | 1,048,576 tokens    |\n| gemini 2.0 flash                 | 1,048,576 tokens    |\n| meta-llama/Llama-3.1-8B-Instruct | 128,000 tokens      |\n\n## Chapter 2\n\n## Performance Metrics\n\nAccuracy, latency , Cost. ![Image 19](C:\\Users\\User\\Projects\\scaled_processing\\data\\documents\\raw\\extracted_images\\A-Comparative-Analysis-of-Leading-Language-Models-for-System-Integration\\picture-19.png)\n\n**AI Analysis** (confidence: 0.9): Here's a detailed description of the image, covering key information, text content, relationships, and technical details:\n\n**Key Information & Insights:**\n\n*   The image illustrates the workflow of a Retrieval-Augmented Generation (RAG) system.",
      "page_number": 11,
      "chunk_index": 21,
      "metadata": {
        "chunk_length": 877,
        "word_count": 82,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "557d93a2807f88f4",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "*   It shows the flow of data and processing steps from a user's question to the final answer. *   RAG enhances a language model by retrieving relevant context from a vector database, which helps in generating more informed and accurate responses. **Readable Text Content:**\n\n*   **Title:** \"How RAG Works?\"\n*   **Boxes:**\n    *   \"QUESTION\"\n    *   \"Vector Database\"\n    *   \"RAG\"\n    *   \"PROMPT\" (containing \"RAG Context\" and \"Question\")\n    *   \"LLM\" (indicating Large Language Model)\n    *   \"ANSWER\" (with a label \"Answer includes RAG context\")\n\n**Relationship to Document:**\n\n*   **User Question:** The process begins with a user posing a question.",
      "page_number": 12,
      "chunk_index": 22,
      "metadata": {
        "chunk_length": 655,
        "word_count": 95,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "06794c2da98f8e35",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "*   **Vector Database:** The question is used to query a vector database, likely to retrieve semantically similar documents or chunks of text. *   **RAG:** The retrieved information becomes the RAG context. *   **Prompt Engineering:** A prompt is constructed by combining the original question with the RAG context. *   **LLM Processing:** The prompt is fed to a Large Language Model (LLM). *   **Answer Generation:** The LLM generates an answer, leveraging both the user's question and the retrieved context.",
      "page_number": 12,
      "chunk_index": 23,
      "metadata": {
        "chunk_length": 509,
        "word_count": 77,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "5bcc460daab74094",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "The answer should ideally incorporate information from the RAG context to provide a more complete and accurate response. **Technical Details (Inferred):**\n\n*   **Vector Database:** This implies using vector embeddings to represent the information in the database, enabling efficient similarity searches. Techniques like cosine similarity are probably used to find relevant documents. *   **RAG:** It entails the retrieval stage is often implemented using algorithms to identify the most relevant chunks of information to be used by the language model.\n\n*   **LLM:** Any LLM can theoretically be used for the task. In summary, the image serves as a high-level visual explanation of the RAG architecture, a prominent technique for improving the performance of language models by integrating external knowledge. ## Testing method:\n\n<!-- image -->\n\n## Accuracy Benchmarks\n\nEvaluating model performance against industry standards. ## 1 - English Set :\n\n<!-- image -->\n\n- Gemini 2.5 flash has the highest accuracy in general\n- Gemini 2.0 flash is quite close to it , even though costing way less\n- on average OpenAI models are performing worse than the gemini family, and cost a bit more.",
      "page_number": 13,
      "chunk_index": 24,
      "metadata": {
        "chunk_length": 1182,
        "word_count": 183,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "854c626be7612357",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "- The local Llama model showed the lowest results , considering it for this project is not viable. This benchmark was done for a set of English questions , that test comprehension , and ability to explain, and tutor. <!-- image -->\n\n## 2- History Set (Malay language):\n\n<!-- image -->\n\n- GPT-4o mini gives better answers in Malay compared to Gemini 2.5 flash, but it struggles to clearly explain those answers. - Gemini 2.5 flash stays strong overall, with more balanced performance between answering and teaching.",
      "page_number": 13,
      "chunk_index": 25,
      "metadata": {
        "chunk_length": 514,
        "word_count": 87,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "568e3c31fd5f0951",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "- Gemini 2.0 flash performs poorly in both answering and teaching, while the local LLaMA model is not usable for Malay at all. This benchmark was done for a set of Malay questions, testing both direct answering and the ability to explain like a tutor. <!-- image -->\n\n## Latency:\n\n<!-- image -->\n\n- In testing, Gemini 2.5 Flash showed higher delays during teaching tasks. - Gemini 2.0 Flash was noticeably faster, with much lower delays in both teaching and answering, while still delivering nearly the same quality.",
      "page_number": 14,
      "chunk_index": 26,
      "metadata": {
        "chunk_length": 516,
        "word_count": 88,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "ea0c77ba6a339154",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "This makes it feel more efficient and refined. - OpenAI9s models were generally the fastest to respond, but earlier benchmarks showed they were less accurate for this teaching task. - The local LLaMA model had very high delays, making it unusable in any interactive setting, whether chat or content generation. <!-- image -->\n\n## Choosing Between Cloud Models and OpenSource Models\n\n## Fine-Tuning\n\n## Cloud Models (e.g., GPT-4o mini, Gemini 2.0/2.5 Flash):\n\nFine-tuning options are limited or not available at all (mostly prompt-based control).",
      "page_number": 14,
      "chunk_index": 27,
      "metadata": {
        "chunk_length": 545,
        "word_count": 84,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "a3780390a1c4d033",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "## Setup Overhead\n\n## Cloud Models:\n\nNo setup required 4 access instantly through an API. ## Flexibility\n\n## Cloud Models:\n\nEasier to scale and integrate; flexibility comes from APIs and built-in features. ## Maintenance\n\n## Cloud Models:\n\nAlways updated by the provider; you automatically use the latest version. ## Open Source (e.g., LLaMA):\n\nCan be fine-tuned extensively, adapted to niche tasks or languages. ## Open Source Models:\n\nRequires infrastructure (GPUs/servers), installation, and optimization.\n\n## Open Source Models:\n\nFull control over weights, training data, and deployment environment. ## Open Source Models:\n\nRequire frequent updates, retraining, and patching; updates can be slow. <!-- image -->\n\n## Which Models Are Most Suitable? 1\n\n## Gemini 2.5 Flash\n\n- Highest overall accuracy in English and Malay. - Balanced between answering and teaching. - Slower response times during teaching tasks. 3\n\n## OpenAI GPT-4o Mini\n\n- Strong in Malay answering. - Weaker in explanations and teaching tasks.",
      "page_number": 15,
      "chunk_index": 28,
      "metadata": {
        "chunk_length": 1014,
        "word_count": 152,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "ec48baf24c96e6ef",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "- Very fast response times. 2\n\n## Gemini 2.0 Flash\n\n- Nearly as strong as 2.5 in English, with lower cost. - Faster and more efficient, but weaker in Malay performance. <!-- image -->\n\n4\n\n## Local LLaMA\n\n- Lowest accuracy in both English and Malay. - Very high latency ³ unusable for interactive use. - Not viable for this project. ## Conclusion\n\nCloud models (Gemini and OpenAI) are the most suitable choice due to their balance of accuracy, speed, and ease of deployment, while local open-source models are not practical.",
      "page_number": 15,
      "chunk_index": 29,
      "metadata": {
        "chunk_length": 523,
        "word_count": 91,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "87afe36d4b184cb6",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "## Moving on to pricing. <!-- image -->\n\n## Cost Breakdown\n\nPricing per 1 million tokens (Input/Output). <!-- image -->\n\n## Why Output Costs More Than Input\n\nWhen using AI language models, it usually costs more to get answers (output) than to give instructions or text (input). This isn9t random4it comes down to how the AI works. - Input (what you type in): The AI can read and process all your text in one go. Think of it like scanning a page4it happens fast and efficiently. - Output (what the AI writes back): The AI doesn9t write everything at once.",
      "page_number": 4,
      "chunk_index": 30,
      "metadata": {
        "chunk_length": 554,
        "word_count": 100,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "7fa3e7ab87f5733a",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "Instead, it creates one word (or piece of a word) at a time. Each new word depends on the previous ones, so the AI has to &lt;think= repeatedly until the full answer is finished. That makes output more expensive. ## What a &lt;Turn= Means in a Conversation\n\nA turn is one back-and-forth between you and the AI. It has two parts:\n\n- Your input 3 this includes your message, plus any background information the system adds. 1. - The AI9s output 3 the response it gives back. 2. A conversation is just a series of these turns, where each new turn builds on the memory of the last ones, so the AI can stay on topic and make sense.",
      "page_number": 16,
      "chunk_index": 31,
      "metadata": {
        "chunk_length": 626,
        "word_count": 119,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "44c959eb8e2e6120",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "<!-- image -->\n\n## How does input, output prices affect our choice? When making an AI system , Each component or model receives different amounts of input , and geneartes different amounts of output , optimizing this is crucial for cost effectiveness. ## 1- RAG:\n\n<!-- image -->\n\nThis diagram explains how RAG (Retrieval-Augmented Generation) works . - You ask a question. 1. - The system looks up relevant information in a special database (like a knowledge library). 2. - That information is added to your question.",
      "page_number": 17,
      "chunk_index": 32,
      "metadata": {
        "chunk_length": 517,
        "word_count": 87,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "09c9c7ed4f9db718",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "3. - The AI then uses both your question and the extra information to give a more accurate and informed answer. 4. So instead of the AI relying only on its memory, it first retrieves supporting facts and then generates a response. ## Analogy\n\nThink of it like asking a friend for help with homework:\n\n- You (the student) : Ask the question. - The friend : Doesn9t just guess4they quickly check a textbook or notes (the database) for useful information. - They combine your question with what they found and explain the answer to you.\n\nThis way, the response is both smarter and more trustworthy. RAG System Cost AnalysisInBalanced vs Context-Heavy Workloads\n\n<!-- image -->\n\n- in a rag system since the context gets provided in the prompt it's an input heavy system. - 80/20 sometimes. - In this case it gives us more freedom and space to use a premium model. - balancing the accuracy and cost effectiveness. ## 2- reacting to enviroment:\n\n<!-- image -->\n\n- Sometimes the AI uses information from its surroundings, like your preferences or details about the situation.",
      "page_number": 17,
      "chunk_index": 33,
      "metadata": {
        "chunk_length": 1068,
        "word_count": 185,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "89ab256f37f998db",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "This is a moderate level of usage 4it9s about half relying on stored knowledge and half on outside context. - If the outside information is simple , a regular model is usually good enough. But if the information is complex or detailed , it9s worth using a more advanced (and more expensive) model to make sure the answers are accurate and reliable. ## 3- Generative:\n\n- When the goal is to generate content like questions, quizzes, or flashcards , the balance is different. It9s mostly about producing output (80%) and only a little about understanding the input (20%).",
      "page_number": 18,
      "chunk_index": 34,
      "metadata": {
        "chunk_length": 569,
        "word_count": 98,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "994f9b4e99b5bf2b",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "- Because generating lots of output is the most expensive part of using AI , cost becomes a key factor to think about in this type of task. ## Correlation, cost and performance:\n\n<!-- image -->\n\n- This scatterplot shows how performance relates to cost. - The Gemini models are performing better than the OpenAI models , while also being more affordable. - The standout is Gemini 2.0 Flash 4its performance is close to Gemini 2.5 Flash but comes at a much lower cost, making it the best value overall.",
      "page_number": 18,
      "chunk_index": 35,
      "metadata": {
        "chunk_length": 500,
        "word_count": 90,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "middle"
      }
    },
    {
      "chunk_id": "54578128180d52b4",
      "document_id": "doc_20250901_185340_9b8f4443",
      "content": "<!-- image -->\n\n## Cost Impact on Model Choice\n\n## Input-Heavy Models (e.g., RAG)\n\n## Balanced Models (e.g., Conversational AI)\n\nLower input token price is key. Gemini 2.5 flash Would reason over context better. Seek low combined input/output cost. Gemini 2.0 flash offers balance, it would be costeffective and have direct instructions. ## Generation-Heavy Models (e.g., Content Creation)\n\nOutput token price dominates. Gemini 2.0 Flash is budgetfriendly for extensive text generation, with the right architecture i can make it way better.",
      "page_number": 19,
      "chunk_index": 36,
      "metadata": {
        "chunk_length": 540,
        "word_count": 81,
        "created_at": "2025-09-01T18:54:46.554347",
        "chunking_strategy": "two_stage_semantic",
        "source_file_path": "data\\documents\\processed\\doc_20250901_185340_9b8f4443\\doc_20250901_185340_9b8f4443_processed.md",
        "original_filename": "doc_20250901_185340_9b8f4443_processed",
        "document_type": "other",
        "chunk_position": "end"
      }
    }
  ],
  "statistics": {
    "total_characters": 26638,
    "total_words": 3907,
    "avg_chunk_length": 719.9,
    "min_chunk_length": 500,
    "max_chunk_length": 1680
  }
}