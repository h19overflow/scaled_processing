"""
Fixed Advanced Docling Demo - Using the correct API methods
"""

import sys
import json
from pathlib import Path

# Add the src directory to Python path
project_root = Path(__file__).parent
src_path = project_root / "src" / "backend"
sys.path.insert(0, str(src_path))

from docling.document_converter import DocumentConverter


def extract_topics_from_markdown(markdown_content: str) -> dict:
    """Extract topics from markdown content - this is the proper way."""
    print(f"\nğŸ“š Topic Extraction from Markdown")
    print("=" * 60)
    
    topics = {}
    current_topic = "Introduction"
    current_content = []
    
    lines = markdown_content.split('\n')
    
    for line in lines:
        line = line.strip()
        
        # Detect headings (lines starting with ##)
        if line.startswith('##') and len(line.split()) < 15:  # Reasonable heading length
            # Save previous topic
            if current_content:
                topics[current_topic] = {
                    "content": current_content,
                    "word_count": sum(len(c.split()) for c in current_content if c.strip()),
                    "line_count": len([c for c in current_content if c.strip()]),
                    "type": "section"
                }
            
            # Start new topic
            current_topic = line.replace('#', '').strip()
            current_content = []
            print(f"   ğŸ“‹ Found topic: {current_topic}")
            
        elif line:  # Non-empty line
            current_content.append(line)
    
    # Don't forget the last topic
    if current_content:
        topics[current_topic] = {
            "content": current_content,
            "word_count": sum(len(c.split()) for c in current_content if c.strip()),
            "line_count": len([c for c in current_content if c.strip()]),
            "type": "section"
        }
    
    # Analyze topic relationships
    if topics:
        longest_topic = max(topics.items(), key=lambda x: x[1]["word_count"])[0]
        shortest_topic = min(topics.items(), key=lambda x: x[1]["word_count"])[0]
        avg_words = sum(t["word_count"] for t in topics.values()) / len(topics)
    else:
        longest_topic = shortest_topic = None
        avg_words = 0
    
    return {
        "topics": topics,
        "topic_count": len(topics),
        "topic_analysis": {
            "longest_topic": longest_topic,
            "shortest_topic": shortest_topic,
            "average_words_per_topic": avg_words,
            "total_words": sum(t["word_count"] for t in topics.values())
        },
        "processing_method": "markdown_parsing"
    }


def extract_semantic_blocks(markdown_content: str) -> dict:
    """Extract semantic blocks from markdown content."""
    print(f"\nğŸ§  Semantic Block Classification")
    print("=" * 60)
    
    semantic_blocks = []
    lines = markdown_content.split('\n')
    
    for i, line in enumerate(lines):
        line = line.strip()
        if not line:
            continue
        
        # Classify block type
        block = {
            "line_number": i + 1,
            "content": line[:200] + "..." if len(line) > 200 else line,
            "word_count": len(line.split()),
            "original_length": len(line)
        }
        
        if line.startswith('##'):
            block["type"] = "heading"
        elif line.startswith('<!-- image -->'):
            block["type"] = "image_reference"
        elif line.startswith('-') or line.startswith('*') or line.startswith('1.'):
            block["type"] = "list_item"
        elif len(line.split()) > 50:
            block["type"] = "long_paragraph"
        elif len(line.split()) > 10:
            block["type"] = "paragraph"
        elif line.isupper() and len(line.split()) < 10:
            block["type"] = "emphasis"
        else:
            block["type"] = "content"
        
        semantic_blocks.append(block)
    
    # Group by type
    grouped_blocks = {}
    for block in semantic_blocks:
        block_type = block["type"]
        if block_type not in grouped_blocks:
            grouped_blocks[block_type] = []
        grouped_blocks[block_type].append(block)
    
    # Print summary
    for block_type, blocks in grouped_blocks.items():
        print(f"   ğŸ“¦ {block_type}: {len(blocks)} blocks")
    
    return {
        "semantic_blocks": semantic_blocks[:50],  # First 50 blocks to avoid huge files
        "grouped_blocks": {k: v[:10] for k, v in grouped_blocks.items()},  # First 10 per type
        "block_types": list(grouped_blocks.keys()),
        "total_blocks": len(semantic_blocks),
        "summary": {block_type: len(blocks) for block_type, blocks in grouped_blocks.items()}
    }


def create_navigation_map(markdown_content: str) -> dict:
    """Create document navigation map."""
    print(f"\nğŸ—ºï¸  Document Navigation Structure")
    print("=" * 60)
    
    navigation_map = {
        "document_outline": [],
        "section_hierarchy": {},
        "navigation_stats": {}
    }
    
    lines = markdown_content.split('\n')
    section_count = 0
    
    for i, line in enumerate(lines):
        line = line.strip()
        
        if line.startswith('##') and len(line.split()) < 15:
            level = len(line) - len(line.lstrip('#'))
            title = line.replace('#', '').strip()
            
            section_info = {
                "title": title,
                "level": level,
                "line_number": i + 1,
                "section_id": section_count,
                "estimated_content_length": 0  # Could calculate this
            }
            
            navigation_map["document_outline"].append(section_info)
            print(f"   {'  ' * (level-1)}ğŸ“ {title}")
            section_count += 1
    
    # Create navigation stats
    navigation_map["navigation_stats"] = {
        "total_sections": len(navigation_map["document_outline"]),
        "heading_levels": {},
        "average_section_title_length": 0
    }
    
    if navigation_map["document_outline"]:
        # Calculate heading level distribution
        for section in navigation_map["document_outline"]:
            level = section["level"]
            if level not in navigation_map["navigation_stats"]["heading_levels"]:
                navigation_map["navigation_stats"]["heading_levels"][level] = 0
            navigation_map["navigation_stats"]["heading_levels"][level] += 1
        
        # Average title length
        total_length = sum(len(section["title"]) for section in navigation_map["document_outline"])
        navigation_map["navigation_stats"]["average_section_title_length"] = total_length / len(navigation_map["document_outline"])
    
    return navigation_map


def run_fixed_demo():
    """Run the fixed demo using proper Docling methods."""
    
    print("=" * 80)
    print("ğŸ”§ Fixed Advanced Docling Features Demonstration")
    print("=" * 80)
    
    # Initialize converter
    converter = DocumentConverter()
    
    # Path to the Gemini PDF
    pdf_path = "src/backend/doc_processing_system/pipelines/document_processing/gemini-for-google-workspace-prompting-guide-101.pdf"
    
    if not Path(pdf_path).exists():
        print(f"âŒ PDF not found: {pdf_path}")
        return
    
    # Create results directory
    results_dir = Path("advanced_docling_results")
    results_dir.mkdir(exist_ok=True)
    
    try:
        print("ğŸš€ Converting document to get markdown content...")
        result = converter.convert(pdf_path)
        markdown_content = result.document.export_to_markdown()
        
        print(f"âœ… Got markdown content: {len(markdown_content):,} characters")
        
        # 1. Extract Topics (The Cool Feature!)
        print("\n" + "="*80)
        topics_data = extract_topics_from_markdown(markdown_content)
        
        with open(results_dir / "topic_groups.json", 'w', encoding='utf-8') as f:
            json.dump(topics_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ Topic analysis complete! Found {topics_data['topic_count']} topics")
        print(f"ğŸ“Š Total words across all topics: {topics_data['topic_analysis']['total_words']:,}")
        
        # 2. Extract Semantic Blocks
        print("\n" + "="*80)
        semantic_data = extract_semantic_blocks(markdown_content)
        
        with open(results_dir / "semantic_blocks.json", 'w', encoding='utf-8') as f:
            json.dump(semantic_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ Semantic analysis complete! Found {semantic_data['total_blocks']} total blocks")
        
        # 3. Create Navigation Map
        print("\n" + "="*80)
        navigation_data = create_navigation_map(markdown_content)
        
        with open(results_dir / "navigation_map.json", 'w', encoding='utf-8') as f:
            json.dump(navigation_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ Navigation map complete! Found {navigation_data['navigation_stats']['total_sections']} sections")
        
        # Show summary
        print("\n" + "="*80)
        print("ğŸ‰ Fixed Demo Results:")
        print(f"   ğŸ“š Topics: {topics_data['topic_count']}")
        print(f"   ğŸ§  Semantic Blocks: {semantic_data['total_blocks']}")
        print(f"   ğŸ—ºï¸  Navigation Sections: {navigation_data['navigation_stats']['total_sections']}")
        print(f"   ğŸ“ All results saved in: {results_dir}")
        
        # Show top topics
        if topics_data['topics']:
            print(f"\nğŸ“‹ Top 5 Topics by Word Count:")
            sorted_topics = sorted(topics_data['topics'].items(), 
                                 key=lambda x: x[1]['word_count'], 
                                 reverse=True)[:5]
            for i, (topic_name, topic_data) in enumerate(sorted_topics, 1):
                print(f"   {i}. {topic_name}: {topic_data['word_count']} words")
        
        print(f"\nâœ¨ Advanced Docling features working perfectly! ğŸ¯")
        
    except Exception as e:
        print(f"âŒ Demo failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    run_fixed_demo()